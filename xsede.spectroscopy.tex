\documentclass[11pt,preprint]{aastex}

%compact captions %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\usepackage[font=small,labelfont=bf, textfont=it, justification=justified]{caption}
%\DeclareCaptionFormat{ruleformat}{\baselineskip0.2cm\hrulefill\\\noindent#1#2#3{\hrulefill}}
%\captionsetup[figure]{format=ruleformat}
\setlength{\textfloatsep}{0pt}
%\setlength{\abovecaptionskip}{-10pt}
\setlength{\floatsep}{2pt}

\usepackage{verbatim}
\usepackage{rotating}
\usepackage{lscape}
\usepackage{subfigure}
\usepackage{natbib}
\usepackage{arydshln}

\usepackage[colorlinks=true, linkcolor=blue, urlcolor=blue, citecolor=blue, pdftex, bookmarks=true, linktocpage=true, hyperindex=true]{hyperref}
\usepackage{breakurl}

\providecommand{\xxxx}{{\color{red} ~XXXX~}}
\providecommand{\xsede}{XSEDE~}
\providecommand{\eg}{e.g.,~}
\providecommand{\ie}{\textit{i.e.},~}
\providecommand{\msun}{M$_\odot$~}

\newcommand{\transpose}[1]{{#1}^{\!\mathsf T}}
\newcommand{\given}{\,|\,}
\renewcommand{\det}[1]{||{#1}||}


\newlength\tindent
\setlength{\tindent}{\parindent}
\setlength{\parindent}{0pt}
\renewcommand{\indent}{\hspace*{\tindent}}

\begin{document}

\section*{\Large Measuring the High-Mass Stellar Initial Mass Function in the Distant Universe}


\begin{center}
Principle Investigator: Dr. Daniel Weisz\\
University of Washington
\end{center}

%{\small
%{\bf abstract:}
%Four years ago, we initiated the single most ambitious mapping project
%ever taken with the Hubble Space Telescope: to produce a map of every
%luminous star in the nearby Andromeda Galaxy (M31) in ultraviolet,
%optical, and near-infrared wavelengths. These $>$117,000,000 stars
%encode the history of the galaxy and the rich, detailed physics that
%drives stellar and galactic evolution. Using our first successful XSEDE
%program, our collaboration pioneered spatially mapping the recent
%history of star formation throughout M31, making a ``movie" of the past
%star formation of the galaxy during the last half a Gigayear.  We now
%propose to extend this movie back to the oldest ages by using a
%forward modeling approach that reproduces the observed colors and
%brightnesses of stars in small cells across the galaxy. We have
%developed both a parameterized model that responds to the added
%complexity of extending our age coverage, and an optimized search
%strategy that allows us to efficiently scan parameter space.  We have
%matched this project to specific XSEDE resources, have meticulously
%optimized our strategy, and have benchmarked our specific analysis
%pipeline.  Combining the computational power of XSEDE with the
%unprecedented number of stars cataloged by our large area survey will
%allow us to derive the most precise spatially-resolved star formation
%history of M31 throughout cosmic time, probing the earliest epochs of
%galaxy formation }


\section{Background \& Motivation}
\label{sec:overview}

A primary goal of astrophysics is to decipher the history of the universe.  Among the most promising approaches has been through the study of galaxies across cosmic time. A galaxy's brightness and color encodes information about its age, rate of star formation, and metal content.  By surveying large collections of galaxies at different cosmic epochs, we are able to trace the evolution of the universe over cosmic timescales.

The use of galaxies as cosmological tools has proliferated over the past two decades.  Large, systematic surveys such as Sloan Digital Sky Survey (SDSS) and the Hubble Ultra-Deep field have provided precise flux measurements of millions of galaxies back to the genesis of galaxy formation around 13 billion years ago.  The success of these programs has motivated the next-generation of astronomical facilities such as the Large Synoptic Survey Telescope, which will survey more than 1 billion galaxies, and the James Webb Space Telescope, which promises to directly detect the formation of the first galaxies.

Yet, despite the wealth of galaxy observations, how we translate their observed light into physical quantities (e.g., mass, metallicity) remains rudimentary.  Typical observations of a distant, unresolved galaxy consist a handful of fluxes measured at different wavelengths that sample the galaxy's spectral energy distribution (SED).  Physically motivated models of galaxies contain many more parameters than typical data points, resulting in significant degeneracies between the model parameters. To mitigate these degeneracies, key assumptions about the form of a galaxy's star formation history (SFH), dust content, and stellar initial mass function (IMF) are necessary.  

The validity of these assumptions can usually be tested using detailed observations of nearby, resolved galaxies.  For example, the dust distribution of hundreds of local galaxies have been used to establish analytic dust ``laws'', which also appear to provide good descriptions of increasingly distant systems.  In the same vein, the simplified SFH models appear to be reasonable approximations.  However, similar tests have not been possible with the stellar IMF, making it the largest source of uncertainty in studies of the distant universe.

The stellar IMF describes the relative distribution of stellar masses for a group of stars born at the same time.  In principle, measuring the IMF is a straight-forward exercise in counting the relative number of individual stars at different masses in a star cluster, i.e., a group of stars that formed at the same time and in the same location.  However, there are two important factors that complicate this measurement in practice.  First is the need for resolved star observations.  Even the high-angular resolution of the Hubble Space Telescope can only resolve individual stars for IMF measurements in the nearest few galaxies, placing several limits on the types of galaxies in which `gold standard' IMF measurements can be made.  This forces us to pursue IMF studies in more distant galaxies, which leads the the second practical challenge.  Measuring the IMF in unresolved systems is hampered by the strong degeneracy between SFH and the IMF.  A multi-age population can emulate fluctuations in shape of the IMF, introducing ambiguity into IMF constraints from galaxies where individual stars cannot be resolved.   Thus, the most reliable IMF measurements are those from direct star counts in the local universe.  As a result, state-of-the-art interpretation of distant galaxies still  relies on IMF from just two local galaxies, the Milky Way (MW) and Large Magellanic Cloud (LMC).  

The consequences of relying on a local IMF to understand interpret the light of distant galaxies are unclear.  On one hand, direct star counts in the MW and LMC do not show evidence of an IMF that depends systematically on local star-forming conditions (e.g., gas density, metallicity, star formation intensity). This could either be because the IMF is ``universal'', i.e., insensitive to environment, or because the local universe has a limited dynamic range of star-forming environments.  If the IMF is universal, then our interpretation of the distant universe is unlikely to dramatically change.  

On the other hand, IMF studies in the slightly more distant universe have hinted at systematic variations that are environmentally dependent.  For example, there have been hints that low-mass, low-metallicity systems form fewer high-mass stars than the current standard IMF.  As the first galaxies also have low masses and metallicities, such IMF variations would force a dramatic revision of how we interpret the very early universe.  At the same time, these measurements rely on interpreting the total light of nearby, but unresolved galaxies, and do not robustly account for degeneracies between IMF and SFH.  Consequently, it is currently impossible to evaluate how well we know one of the most fundamental parameters necessary to understand the history of the universe. 


\section{Measuring the High-Mass IMF in the Distant Universe}

To address the current shortcomings with the IMF, we are developing a qualitatively new method to measure the IMF in unresolved, distant and star-forming galaxies.  Due to their high luminosities, these types of galaxies typically detected at all cosmic epochs.  In such systems, stars more massive than the sun determine the galaxy SEDs. Therefore we are focused on the IMF in this mass regime above 1 \msun (``the high-mass IMF'').

Our approach to measuring the IMF in distant galaxies is through their young star cluster populations. In the local universe, counting stars as a function of their mass in young clusters is the optimal way to measure the IMF.  By definition, clusters are single-age populations, which eliminates the degeneracy between IMF and SFH.  However, the direct star count technique is limited to the local universe, and instead we must turn to other methods to infer the IMF in the distant universe.  

For most distinct clusters, we are measuring the IMF from their spectrum.  The full optical spectrum of a young ($<$ 50 Myr) star cluster contains significant information about the slope of the high mass IMF.  The shape of the blue continuum and size of spectral features (e.g., nebular emission, the Balmer jump) are sensitive to the slope of the high mass IMF.  In comparison, traditional analysis of star clusters relies on isolating key absorption features and modeling their relative intensities.  This type of spectrophotometric index analysis is well-known to be insensitive to the IMF\citep[e.g.,][]{kol08}, and thus has never been used as means to investigate the IMF.  

In contrast, our approach is to forward model the full optical spectrum of a young star cluster.  That is, we use state-of-the-art population stellar population synthesis software to construct the expected spectrum of a young cluster and build a separate noise model to account for observational uncertainties introduced by the resolution of the telescope, inefficient of the charged-coupling device, atmospheric turbulence, sky line emission, etc.  Many of these noise features are non-linear and require involved noise models (e.g., Gaussian Processes) as we described in \S \ref{sec:calibration}.  The result of this new approach to modeling is a complete model of the observed cluster spectrum, which will, for the first time, enable direct measurements of they high-mass IMF in star-forming galaxies outside the very local universe.  

\subsection{Validating Spectroscopic IMF constraints in M31}

Our program currently consists of two phases.  The first phase is development and validation of the methodology and the second is a prototype application to a moderately distant galaxy.

Following successful trials of the code on simulated data, we are now validating our spectroscopic IMF constraints against `gold standard' IMF measurements from resolved star counts.  As part of the Panchromatic Hubble Andromeda Treasury, a 4 year Hubble Space Telescope campaign to resolve stars in our closest neighbor, the Andromeda galaxy, PI Weisz has measured the IMF in $>$500 star clusters using resolved star counts.  Over the past two years, we have also acquired high-fidelity spectra of $\sim$ 50 of the most luminous young clusters with the \textit{MMT} and \textit{Keck} telescopes in order to spectroscopically measure their IMFs and compare the results with resolved star analysis.  

This project will provide an empirical validation of our spectroscopic technique.  This step is key in both assessing the reliability of our results and in demonstrating the methodology to the broader astronomical community.  Because we are modeling all physical properties of a cluster, we will compare constraints on all physical parameters (such as cluster age, mass, IMF slope) between the resolved star and spectroscopic  methodologies.  The development of this methodology alone promises to revolutionize the use of young star clusters for studying the distant universe.

\subsection{The First High-Mass IMF Measurement Outside the Local Group}

The second phase of our program is a pilot study of the IMF in NGC~628.  NGC~628 is an ideal prototype as it is a moderately distant (D $\sim$ 9 Mpc), face-on spiral galaxy that hosts a rich young cluster population (Larsen et al. 1999).  It has a SFR that is $\sim$10 times higher than M31, which provides a different environment in which to study the IMF.  Additional benefits include an extensive collection of ancillary data (e.g., Hubble Space Telescope imaging, gas maps from the Very Large Array) that can be used to search for environmental variations of the IMF within NGC~628. Interestingly, it also hosts the highest frequency of recent supernovae (SNe; 3 in the last decade;  2002ap, 2003gd, 2013ej) of any galaxy within 10 Mpc, which could be due to an IMF that favors the formation of more massive massive stars. 

In late-2014, we obtained high signal-to-noise spectroscopy of $\sim$ 30 young clusters in NGC~628 using \texttt{Keck} telescope and plan to measure the IMF of each cluster following the completing of the program's first phase.  Using our NGC~628 results as a prototype, we will pursue systematic measurements of the high-mass IMF in increasingly distant and diverse galaxies.

\subsection{Future Development}

Although the primary these of our programs has been to the measure the high-mass IMF, the general spectroscopic modeling framework is applicable to a wide variety of astrophysical contexts.  Of particular interest is in modeling the integrated light of galaxies with relaxed assumptions of the SFH.  As discussed in \S \ref{XXX}, strong assumptions for the analytic form of the SFH are commonly used to mitigate degeneracies with other physical parameters.  However, within the framework of this code, we can directly test the validity of these assumptions both on galaxies with only broad SED (currently most common) and those with integrated spectroscopy.  Some similar explorations exist in the astronomical literature, but none at the level of detail we propose.  Specifically, our flexible noise model is qualitatively new and maximizes the amount of information used in the modeling of clusters and galaxies, whereas conventional methods (e.g., spectrophotometric index modeling) only retain a small fraction of the total dataset.  Using the general framework developed for IMF investigations, we plan to explore broader implications for the general modeling of distant galaxies.


\section{Potential Scientific Impact}

The high-mass IMF underpins virtually all of galaxy and cosmological studies that rely on star-forming galaxies, and this work is the first to systematic measure the IMF outside of the immediate universe.  The concrete scientific outcomes of this program include the introduction of a new and widely applicable technique of analyzing spectroscopy.  In the context of the IMF, this will be transformative, as it extends our ability to search for IMF variations by orders of magnitude is galaxy distance.  In a broader sense, the analysis framework will provide a baseline by which astronomers from a variety of unrelated fields can improve spectroscopic modeling.  It is clearly applicable to galaxy astrophysics, but has the potential to transform how stellar astrophysics is done, which is particular significant in light of the heavy investment in new facilities designed to spectroscopically survey the Milky Way (e.g., APOGEE).

The prototype study in NGC~628 is equally critical. Measuring the IMF in NGC~628 represents the first concrete direct of the high-mass IMF outside the very local universe and will motive a wealth of future studies.

The long-term scientific impact of our programs is has the potential to be truly transformative.  For example, if we find that the IMF varies with respect to environment, virtually all of galaxy astrophysics, chemical evolution of the universe, and galaxy-based cosmology will have to be revised.  This program is the first step in a much longer quest to understand the broader evolution of the universe.

\section{Methodology}

Information about the properties of a stellar cluster -- its age, metallicity, total stellar mass, redshift, and reddening by dust among many others -- is contained in the detailed spectrum and the broadband SED of the cluster. 
Our code performs inference of these star cluster parameters by comparing model spectra and photometry to observations for a broad range of model parameters.  
This is accomplished in the framework of a likelihood function for the data given the model parameters.  

The high dimensionality of the parameter space, the desire to robustly infer degeneracies between the parameters, the presence of informative prior information about the parameters, and the need marginalize over ``nuisance'' parameters all lead us to a Bayesian methodology based on Markov Chain Monte Carlo (MCMC) sampling of the posterior probability distribution for the parameters. 
The combination of photometric and spectroscopic information requires a flexible noise model to account for spectroscopic calibration uncertainties.

The heart of our star cluster modeling code consists of the \texttt{FSPS} stellar population synthesis code \citep{fsps}, combined with a flexible spectroscopic calibration model.  
For the MCMC sampling we are using the affine-invariant ensemble sampler \texttt{emcee} \citep{emcee} which enables the likelihood function evaluations to parallelized across a large number CPUs. 
%The main calculations of \texttt{FSPS} are done in Fortran, but the code is wrapped in Python.  \texttt{emcee} is written in Python and uses \texttt{mpi4py} to distribute likelihood calcualtions across CPUs.

\subsection{Star cluster model}
For each set of model parameters a star cluster model must be generated and compared to the data. 
%The star cluster model is comprised of a physical model for the spectrum and broadband SED of and a model for the instrumental calibration.  
The physical model is generated using the state-of-the-art FSPS stellar population synthesis code.
FSPS combines tabulated model stellar spectra according to weights determined from tabulated stellar evolutionary tracks and an IMF.
The tabulated data are loaded into memory at the start of program execution, and there is very little disk I/O in our entire pipeline.
The FSPS code additionally calculates the reddening due to dust and convolves the spectrum with a broadening function representing the instrumental spectral resolution.  
Finally, broadband photometry is generated by projecting the model spectrum onto the observational filters


%The model generation takes approximately 1s on 

\subsection{Calibration modeling}
\label{sec:calibration}
The absolute flux calibration of spectroscopic data is generally inferior to that of photometric data.  
While spectroscopic features such as absorption line depths provide substantial information about the stellar population parameters, large scale features in the observed spectrum and its overall normalization are often affected by substantial (multiplicative) calibration uncertainties. 
Accuratelely marginalizing over the uncertain calibration uncertainties is critical for obtaining unbiased constraints on the IMF. 
We have therefore included a very flexible spectroscopic calibration model based on a combination of a low-order polynomial function of wavelength and a Gaussian Process.

The low order polynomial corresponds to gross features in the calibration as a function of wavelength.
The Gaussian Process corresponds to smaller scale deviations away from the polynomial, which can equivalently be thought of as covariant uncertainties on the fluxes of datapoints close in wavelength.

The likelihood function that we are suing for the spectroscopic data is 
\begin{eqnarray}\label{eq:spectroscopicLF}
\ln p(s\given\theta,\alpha) &=& -\frac{1}{2}\,\left[\transpose{\Delta}\,C^{-1}\,\Delta + \ln([2\pi]^N\,\det{C}) \right]
\\
\Delta &\equiv& \ln s - \ln \mu(\theta) - f(\alpha) 
\\
C_{nn'} &\equiv& \sigma_n^2 \,\delta_{nn'} +
K_\alpha(\lambda_n - \lambda_{n'})
%\\
%\hat{\sigma_n} & = &\sigma_n/d
\quad ,
\end{eqnarray}
where $s$ is the the observed spectrum as a vector,
$\theta$ are the physical parameters of the cluster,
$\alpha$ are the (nuisance) calibration parameters
(which determine the the shape of the polynomial calibration vector and the covariance kernel function),
$\Delta$ is a residual vector,
$\mu(\theta)$ is the modeled spectrum as a vector,
$f(\alpha)$ is the polynomial calibration vector,
$C$ is the Gaussian Process covariance matrix,
made up of $N^2$ values $C_{nn'}$,
$K_\alpha$ is a kernel function that sets the wavelength scale and
amplitude of the residual calibration issues,
$\sigma_n$ are the fractional uncertainties of the data,
and each $\lambda_n$ is the wavelength of spectral pixel $n$.
We invert the $N \times N$ matrix $C$ using Cholesky decompositions, as implemented in \texttt{Scipy}. 
For our data, N$\sim 2500$.  
On Stampede the matrix inversion takes approximately 1s on a single processor, using the linked Intel MKL library.

Meanwhile, the likelihood function that we are using for the photometry is more straightforward, 
\begin{eqnarray}\label{eq:photometricLF}
\ln p(d\given\theta,j^2) &=& -\frac{1}{2}\,\sum_{n=1}^N \left[\frac{[d_n - \mu_n(\theta)]^2}{[\sigma_n^2 + j^2]} + \ln(2\pi\,[\sigma_n^2 + j^2]) \right]
\quad ,
\end{eqnarray}
where $d_n$ are the $N$ broadband photometric measurements,
$\theta$ are again the physical parameters of the cluster,
$\mu(\theta)$ are the modeled photometry,
made up of $N$ values $\mu_n(\theta)$,
the $\sigma_n^2$ are the observational noise variances,
and $j$ is a parameter allowing for the possibility of additional noise above that reported.
These two likelihoods are summed and multiplied by the prior probabilities for each parameter ${\theta, \alpha}$ to determine the posterior probability.


\subsection{MCMC Sampling}
For our MCMC sampling of the $\sim 20$ parameters $\{\theta, \alpha\}$ we are using the code \texttt{emcee}.  
This code implements an affine-invariant ensemble sampler \citep{goodman_weare} which is based on an ensemble of ``walkers'' in parameter space that, at each iteration of the sampler, are used to generate new proposal positions in parameter space.  
This eliminates the need for hand-tuning of the proposal length scales.  
Additionally, the likelihood calculations for the different walkers at a given iteration can be simply parallelized.

As described in the code performance document, all parallelization of the code is acheived by parallelizing the likelihood calls in \texttt{emcee}, and extremely good parallel efficiency can be obtained when the ensemble of ``walkers'' is large.


%In cases where the walkers in the sampler have not converged to a stationary distribution in parameter space, we can restart the sampler from the last ensemble of positions, thus taking advantage of previous computations.  
%We are investigating techniques for robust online assessment of sampler convergence.

%Parallelization is accomplished as follows. 
%The parameter position of each walker is sent to a separate processor for the likelihood evaluation. 
%The likelihoods are then collected and used to generate new proposed parameter positions for each walker, and a new iteration is begun.  
%The density of walker positions constitutes an approximation to the posterior probability distribution for the parameters.

%\section{Justification of Resources Requested}
%Our tests indicate that for each spectrum approximately X iterations of Y walkers are required for convergence of a sampler.  This number of likelihood calculations requires 

%The observational data is preprocessed using custom software on local compute resources.


%\subsection{Observational Data}
%Preprocessing and uploading to Stampede

%\subsection{Job submission}
%We will submit a separate job for each cluster spectrum. normal queue

%\subsection{Job details}
%Each CPU loads a copy of the likelihood function into memory, including the FSPS star cluster modelling code and the spectroscopic calibration model.

%For each spectrum, an initial round of optimization of the likelihood function (actually the posterior probability function) is used to find the approximate maximum likelihood. 
%This optimization uses Powell's method as implemented in \texttt{Scipy}, though we are investigating more efficient algorithms. 
%The optimization is started from as many positions as there are CPUs available in the job, with each CPU  performing one realization of the optimization.  
%Thus while increasing the number of processors does not speed up this phase of the code, it does substantially reduce the chance of becoming tapped in a local minimum.

%After the separate optimizations have each converged or reached a maximum number of iterations, the parameters corresponding to the global minimum are used as a central location for the MCMC sampling.
%An ensemble of ``walkers'' is then generated centered on this location and evolved forward.  
%The resulting 

%\subsection{Post-Processing}
%The stored walker positions are transferred to local servers to run our custom diagnostic and visualization software.  
%In cases where the walkers in the sampler have not converged to a stationary distribution in parameter space, we can restart the sampler from the last ensemble of positions, thus taking advantage of previous computations.  
%We are investigating techniques for robust online assessment of sampler convergence.


\section{Resource Request \& Justification}

For our startup allocation, we requested time on both Trestles and Stampede, but were ultimately only given time on Trestles.  This proved to be problematic.  Jobs that began running on Trestles would die in the midst of sampling with no warning.  We spent over 2 months working with Trestles staff to identify the cause of the issue, but there has been no resolution to date.  We transferred $\sim$ 25\% of our Trestles allocation to Stampede and have found great success on Stampede.  Stampede is fully optimized for the problem at hand, outperforming any of our local resources at UW or Harvard by factors of $\sim$5-10 in time per likelihood call.  We therefore request use of Stampede for this program.

Overall, we estimate that each cluster will require XXX SUs for full convergence.  This estimate includes both the initial Powell optimization as well as the burn-in and final MCMC chains.  For the 80 clusters in M31 and NGC~628 we request a total of XXX SUs.  We also request an additional 50k SUs for development of our generalized SFH modeling routines.  \textbf{XXX something about MICs XXX}.  Our request is summarized in Table \ref{tab1}.

\begin{table}[h!]
\begin{center}
\begin{tabular}{cc|c}
Project & Clusters & SUs \\
\hline
M31 & 80 & XXX \\
NGC~628 & 30 & XXX \\
Development & & XXX \\
\hline Total SUs & & YYY \\
\hline

\end{tabular}
\end{center}
\label{tab1}
\end{table}%


Due to the relative small size of our data, we do not request any additional storage allocation on top of the standard file system provided on Stampede.  Longer term storage will be done at Washington and Harvard as needed.





% \end{deluxetable}



\section{Team}

There are three team members working on this project.  All three worked together in the Astronomy Department at the University of California at Santa Cruz in 2013 and 2014, prior to moving to their current institutions.

\textbf{Daniel Weisz:}  Principle Investigator and Lead Scientist.  PI Weisz is currently a Hubble Fellow at the University of Washington.  Previously he held postdoctoral research positions at UC Santa Cruz and the University of Washington.  The focus of his Hubble Fellowship project is on searching for variations in the high-mass IMF in the distant universe.  PI Weisz holds a Ph.D. in astrophysics from the University of Minnesota, has published 14 first author, including a seminal measurement of the high-mass IMF in M31, and over 75 total peer reviewed papers. He is an expert on local galaxies and star-formation, and is leading efforts to explore new ways to measure the high-mass IMF.  \\

\textbf{Benjamin Johnson:} Co-Investigator and Lead Software Developer.  Co-I Johnson is currently a postdoctoral research at the Center for Astrophysics at Harvard University.  Co-I Johnson holds a Ph.D. in astronomy from Columbia University and has previously been a postdoctoral researcher at UC Santa Cruz, the CNRS in Paris, and Cambridge University.  co-I Johnson is the author of five first author papers and 50 peer reviewed publications in total.  He is an expert in galaxy formation and evolution and in analysis of SED and galaxy spectroscopy.  \\

\textbf{Charlie Conroy:}  Co-Investigator and Creator of FSPS.  Co-I Conroy is a new assistant faculty member in the Department of Astronomy at Harvard University.  Previously he was an assistant faculty member at UC Santa Cruz and held postdoctoral positions at Princeton, Harvard, and the University of Chicago.  He holds a Ph.D. in astrophysics from Princeton.  Co-I Conroy is the author of FSPS, the spectral synthesis code that underpins the physical models of this project and has published several highly cited papers about modeling the integrated light in galaxies and variations in the low-mass IMF.  He is an expert in stellar and galactic astrophysics and in modeling the integrated light of galaxies. \\



\bibliographystyle{apj}
\bibliography{references}



\end{document}
