\documentclass[11pt,preprint]{aastex}

%compact captions %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\usepackage[font=small,labelfont=bf, textfont=it, justification=justified]{caption}
%\DeclareCaptionFormat{ruleformat}{\baselineskip0.2cm\hrulefill\\\noindent#1#2#3{\hrulefill}}
%\captionsetup[figure]{format=ruleformat}
\setlength{\textfloatsep}{0pt}
%\setlength{\abovecaptionskip}{-10pt}
\setlength{\floatsep}{2pt}

\usepackage{verbatim}
\usepackage{rotating}
\usepackage{lscape}
\usepackage{subfigure}
\usepackage{natbib}
\usepackage{arydshln}

\usepackage[colorlinks=true, linkcolor=blue, urlcolor=blue, citecolor=blue, pdftex, bookmarks=true, linktocpage=true, hyperindex=true]{hyperref}
\usepackage{breakurl}

\providecommand{\xxxx}{{\color{red} ~XXXX~}}
\providecommand{\xsede}{XSEDE~}
\providecommand{\eg}{e.g.,~}
\providecommand{\ie}{\textit{i.e.},~}

\begin{document}

\section*{\Large Toward Measuring the Stellar Initial Mass Function in the Distant Universe}


\begin{center}
Principle Investigator: Dr. Daniel Weisz\\
University of Washington
\end{center}

{\small
{\bf abstract:}
Four years ago, we initiated the single most ambitious mapping project
ever taken with the Hubble Space Telescope: to produce a map of every
luminous star in the nearby Andromeda Galaxy (M31) in ultraviolet,
optical, and near-infrared wavelengths. These $>$117,000,000 stars
encode the history of the galaxy and the rich, detailed physics that
drives stellar and galactic evolution. Using our first successful XSEDE
program, our collaboration pioneered spatially mapping the recent
history of star formation throughout M31, making a ``movie" of the past
star formation of the galaxy during the last half a Gigayear.  We now
propose to extend this movie back to the oldest ages by using a
forward modeling approach that reproduces the observed colors and
brightnesses of stars in small cells across the galaxy. We have
developed both a parameterized model that responds to the added
complexity of extending our age coverage, and an optimized search
strategy that allows us to efficiently scan parameter space.  We have
matched this project to specific XSEDE resources, have meticulously
optimized our strategy, and have benchmarked our specific analysis
pipeline.  Combining the computational power of XSEDE with the
unprecedented number of stars cataloged by our large area survey will
allow us to derive the most precise spatially-resolved star formation
history of M31 throughout cosmic time, probing the earliest epochs of
galaxy formation }


\section{Background \& Motivation}
\label{sec:overview}

A primary goal of astrophysics is to decipher the history of the universe.  Among the most promising approaches has been through the study of galaxies across cosmic time. A galaxy's brightness and color encodes information about its age, rate of star formation, and metal content.  By surveying large collections of galaxies at different cosmic epochs, we can measure how the universe has changed as a function of time.  

The use of galaxies as cosmological tools has proliferated over the past two decades.  Large, systematic surveys such as Sloan Digital Sky Survey (SDSS) and the Hubble Ultra-Deep field have provided precise flux measurements of millions of galaxies back to the genesis of galaxy formation around 13 billion years ago.  The success of these programs has motivated the next-generation of astronomical facilities such as the Large Synoptic Survey Telescope, which will survey more than 1 billion galaxies, and the James Webb Space Telescope, which promises to directly detect the first galaxies.

Yet, despite the wealth of galaxy observations, how we translate their observed light into physical quantities (e.g., mass, metallicity) remains rudimentary.  Typical observations of a distant, unresolved galaxy consist a handful of fluxes measured at different wavelengths that sample the galaxy's spectral energy distribution (SED).  Physically motivated models of galaxies contain many more parameters than typical data points, resulting in significant degeneracies between the model parameters. To mitigate these degeneracies, key assumptions about the form of a galaxy's star formation history (SFH), dust content, and stellar initial mass function (IMF) are necessary.  

The validity of these assumptions can usually be tested using detailed observations of nearby, resolved galaxies.  For example, the dust distribution of hundreds of local galaxies have been used to establish analytic dust ``laws'', which also appear to provide good descriptions of increasingly distant systems.

However, similar tests have not been possible with the stellar IMF.  The stellar IMF describes the relative distribution of stellar masses for a group of stars born at the same time.  In principle, measuring the IMF is a straight-forward exercise in counting the relative number of individual stars at different masses in a star cluster, i.e., a co-eval stellar population.  However, there are two important factors that complicate this measurement in practice.  The first is the need for resolved stars.  Even the high-angular resolution of the Hubble Space Telescope can only resolve individual stars for IMF measurements in the nearest few galaxies.  Second, measuring the IMF in more distant galaxies where individual stars cannot be resolved is challenging due to the strong degeneracy between SFR and the IMF.  As a result, state-of-the-art interpretation of distant galaxies still  relies on IMF from just two local galaxies, the Milky Way (MW) and Large Magellanic Cloud (LMC).  

The consequences of relying on a local IMF to understand interpret the light of distant galaxies are unclear.  On one hand, direct star counts in the MW and LMC do not show evidence of an IMF that depends systematically on local star-forming conditions (e.g., gas density, metallicity, star formation intensity). This could either be because the IMF is ``universal'', i.e., insensitive to environment, or because the local universe has a limited dynamic range of star-forming environments.  In contrast, IMF studies in the slightly more distant universe have hinted at systematic variations more in line with expectations from traditional theories of star-formation.  However, these measurements rely on interpreting the total light of nearby, but unresolved galaxies, and do not robustly account for degeneracies between IMF and SFH.  In short, the IMF is challenging to accurately measure and remains one of the largest sources of uncertainty in our use of galaxies as proxies of cosmology.


\section{Measuring the High-Mass IMF in the Distant Universe}

We are developing a qualitatively new method to measure the IMF in unresolved, distant and star-forming galaxies.  Due to their high luminosities, these are the types of galaxies typically detected at all cosmic epochs.  In these system, it is stars more massive than the sun that determine their SEDs, and thus we are focused on the IMF in this mass regime (``the high-mass IMF'').

Our approach to measuring the IMF in distant galaxies is through their young star cluster populations. In the local universe, counting stars as a function of their mass in young clusters is the optimal way to measure the IMF.  By definition, clusters are single-age populations, which eliminates the degeneracy between IMF and SFH.  However, the direct star count technique is limited to the local universe, and instead we must turn to other methods to infer the IMF in the distant universe.  

For most distinct clusters, we are measuring the IMF using their spectrum.  The full optical spectrum of a young ($<$ 50 Myr) star cluster contains significant information about the slope of the high mass IMF.  The shape of the blue continuum and size of spectral features (e.g., nebular emission, the Balmer jump) are sensitive to the slope of the high mass IMF.  In comparison, traditional analysis of star clusters relies on isolating key absorption features and modeling their relative intensities.  This type of spectrophotometric index analysis is well-known to be insensitive to the IMF\citep[e.g.,][]{kol08}, and thus has never been used as means to investigate the IMF.  

In contrast, our approach is to forward model the optical spectrum of a young star cluster.  That is, we use state-of-the-art population stellar population synthesis software to construct the expected spectrum of a young cluster and build a separate noise model to account for observational uncertainties introduced by the resolution of the telescope, inefficient of the charged-coupling device, atmospheric turbulence, sky line emission, etc.  Many of these noise features are non-linear and require involved noise models (e.g., Gaussian Processes) as we described in \S \ref{XXX}.  The result of this new approach to modeling is that we will, for the first time, be able to make unbiased measurements of the IMF outside the very local universe.  

\subsection{Validating Spectroscopic IMF constraints in M31}

Our program is being conducted in two phases.  The first phase is development and validation of the methodology and the second is a prototype application to a moderately distant galaxy.

Following successful trials of the code on simulated data, we are now validating our spectroscopic IMF constraints against `gold standard' IMF measurements from resolved star counts.  As part of the Panchromatic Hubble Andromeda Treasury, a 4 year Hubble Space Telescope campaign to resolve stars in our closest neighbor, the Andromeda galaxy, PI Weisz has measured the IMF in $>$500 resolved star clusters.  At the same time, we have also acquired high fidelity spectra of $\sim$ 50 of these same star clusters using the 6.5m Multi-Mirror Telescope in Arizona and the 10m Keck Telescope in Hawaii.  

Using our spectroscopic analysis tools, we are measuring the physical properties (e.g., age, mass, IMF) of each cluster from its integrated spectrum and comparing it to the ground-truth values derived from resolved star counts.  Consistency between these two sets of parameters will provide key validation of the spectroscopic technique. 





\subsection{The First High-Mass IMF Measurement Outside the Local Group}

NGC~628 is an ideal target for a pilot study.  It is a nearby (D $\sim$ 9 Mpc), face-on spiral galaxy that hosts a rich young cluster population (Larsen et al. 1999). It has a recent star formation rate (SFR) of $\sim$ 2- 3 \msun /yr (Sanchez et al.\ 2011) making it an excellent Milky Way analog.  Further, NGC~628 hosts the highest frequency of recent supernovae (SNe; 3 in the last decade;  2002ap, 2003gd, 2013ej) of any galaxy within 10 Mpc.  This high recent SNe rate may indicate that NGC 628 has a higher than normal expected massive star population, which could be due to a recent burst or the possibility of a top-heavy IMF that could also produce a larger number of massive SNe progenitors relative to a standard IMF.

Studying the IMF in NGC~628 provides a nice comparison for our ongoing IMF work in M31.  Compared to M31, NGC~628 is more actively forming stars 3 vs. 0.5 \msun\ / yr), is located in the blue cloud instead of the green valley, and has clear undisturbed spiral morphology, which is more typical than the star-forming `rings' of M31.  NGC~628 also provides a better observational target for this study. M31 hosts only a handful of massive young clusters ($\sim$ 10$^4$ \msun).  Lower mass clusters are subject to stochastic sampling of the IMF effects that are challenging to efficiently include spectrophotometric fitting.  Measuring the IMF in higher mass clusters mitigates stochastic IMF sampling, providing more secure IMF determinations.

This program is the first step in a larger planned LRIS survey to systematic search for environmentally sensitivity of the upper IMF.  NGC~628 is an excellent template in which we can learn about the IMF in a Milky Way analog, demonstrate the capabilities of this approach to the broader community, and efficiently enable us to refine our reduction and analysis pipelines (e.g., LRIS exposure times, HST photometric reductions, etc) before pursuing a larger systematic LRIS study of the IMF.



\section{Potential Science Impact}



\section{Methodology}

Information about the properties of a stellar cluster -- its age, metallicity, total stellar mass, and reddening by dust among many others -- is contained in the detailed spectrum and the broadband SED of the cluster. 
Our code performs inference of these star cluster parameters by comparing model spectra and photometry to observations for a broad range of model parameters.  
This is accomplished in the framework of a likelihood function for the data given the model parameters.  

The high dimensionality of the parameter space, the desire to robustly infer degeneracies between the parameters, the presence of informative prior information about the parameters, and the need marginalize over ``nuisance'' parameters all lead us to a Bayesian methodology based on Markov Chain Monte Carlo (MCMC) sampling of the likelihood function. 
The combination of photometric and spectroscopic information requires a flexible noise model to account for spectroscopic calibration uncertainties.

The heart of our star cluster modeling code consists of the \texttt{FSPS} stellar population synthesis code \citep{fsps}, combined with a flexible spectroscopic calibration model.  
For the MCMC sampling we are using the affine-invariant ensemble sampler \texttt{emcee} \citep{emcee} which enables the likelihood function evaluations to parallelized across a large number CPUs. 
The main calculations of \texttt{FSPS} are done in Fortran, but the code is wrapped in Python.  \texttt{emcee} is written in Python and uses \texttt{mpi4py} to distribute likelihood calcualtions across CPUs.

\subsection{Star cluster model}
For each set of model parameters a star cluster model must be generated and compared to the data. 
The star cluster model is comprised of a physical model for the spectrum and broadband SED of and a model for the instrumental calibration.  
The physical model is generated using the FSPS stellar population synthesis code, which combines tabulated model stellar spectra according to weights determined from tabulated stellar evolutionary tracks and an initial mass function.  
The FSPS code additionally calculates the reddening due to dust and convolves the spectrum with a broadening function representing the instrumental spectral resolution.

The model generation takes approximately 1s on 

\subsubsection{Calibration modeling}
The absolute flux calibration of spectroscopic data is generally inferior to that of photometric data.  
While spectroscopic features such as absorption line depths provide substantial information about the stellar population parameters, large scale features in the observed spectrum and its overall normalization are often affected by substantial (multiplicative) calibration uncertainties. 
In order to simultaneously model the observed spectra and the photometry we have included a very flexible spectroscopic calibration model based on a combination of a low-order polynomial function of wavelength and a Gaussian Process.

The low order polynomial corresponds to gross features in the calibration as a function of wavelength, while the Gaussian Process corresponds to smaller scale deviations away from the polynomial, which can be thought of as covariant uncertainties on the fluxes of datapoints close in wavelength.

Determining the likelihood in the presence of the covariant uncertainties requires inverting $N \times N$ matrices, where $N$ is the number of wavelength points.  
This is accomplished using Cholesky decompositions, as implemented in \texttt{Scipy}.  
For our data, N$\sim 2500$.  
On Stampede the matrix inversion takes approximately 1s on a single processor, using the linked Intel MKL library.

\subsection{MCMC}
For our MCMC sampling we are using the code \texttt{emcee}.  
This code implements an affine-invariant ensemble sampler \citep{goodman_weare} which is based on an ensemble of ``walkers'' in parameter space that, at each iteration of the sampler, are used to generate new proposal positions in parameter space.  
This eliminates the need for hand-tuning of the proposal length scales.  
Additionally, the likelihood calculations for the different walkers at a given iteration can be simply parallelized.

Parallelization is accomplished as follows. 
The parameter position of each walker is sent to a separate processor for the likelihood evaluation. 
The likelihoods are then collected and used to generate new proposed parameter positions for each walker, and a new iteration is begun.  
The density of walker positions constitutes an approximation to the posterior probability distribution for the parameters.

\section{Justification of Resources}
Our tests indicate that for each spectrum approximately X iterations of Y walkers are required for convergence of a sampler

\subsection{Observational Data}
Preprocessing and uploading to Stampede

\subsection{Job submission}
We will submit a separate job for each cluster spectrum. normal queue

\subsection{Job details}
Each CPU loads a copy of the likelihood function into memory, including the FSPS star cluster modelling code and the spectroscopic calibration model.

For each spectrum, an initial round of optimization of the likelihood function (actually the posterior probability function) is used to find the approximate maximum likelihood. 
This optimization uses Powell's method as implemented in \texttt{Scipy}, though we are investigating more efficient algorithms. 
The optimization is started from as many positions as there are CPUs available in the job, with each CPU  performing one realization of the optimization.  
Thus while increasing the number of processors does not speed up this phase of the code, it does substantially reduce the chance of becoming tapped in a local minimum.

After the separate optimizations have each converged or reached a maximum number of iterations, the parameters corresponding to the global minimum are used as a central location for the MCMC sampling.
An ensemble of ``walkers'' is then generated centered on this location and evolved forward.  
The resulting 

\subsection{Post-Processing}
The stored walker positions are transferred to local servers to run our custom diagnostic and visualization software.  
In cases where the walkers in the sampler have not converged to a stationary distribution in parameter space, we can restart the sampler from the last ensemble of positions, thus taking advantage of previous computations.  
We are investigating techniques for robust online assesment of sampler convergence.






% \end{deluxetable}

\clearpage

\section{Team}

There are three primary team members working on this project:

\textbf{Daniel Weisz:} \\

\textbf{Benjamin Johnson:} \\

\textbf{Charlie Conroy:}



\bibliographystyle{apj}
\bibliography{references}



\end{document}
